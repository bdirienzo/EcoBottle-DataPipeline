# â™»ï¸ EcoBottle AR â€“ Data Warehouse & BI Dashboard

## ðŸ“Š DescripciÃ³n general

**EcoBottle AR** es un proyecto acadÃ©mico de **Data Warehouse & Business Intelligence** que implementa un miniâ€“ecosistema de datos comercial **(online + offline)** para una empresa ficticia dedicada a la venta de botellas reutilizables.

El objetivo principal es diseÃ±ar e implementar un **pipeline ETL reproducible en Python**, construir un **modelo estrella (Kimball)** en **SQLite**, y desarrollar un **dashboard analÃ­tico en Looker Studio** que permita monitorear los indicadores clave del negocio.

El proyecto integra mÃºltiples fuentes de informaciÃ³n (ventas, pagos, envÃ­os, sesiones web y encuestas NPS) para generar una vista consolidada del desempeÃ±o comercial y de la experiencia del cliente.  

Este repositorio contiene el cÃ³digo, los datos y la documentaciÃ³n necesaria para reproducir todo el flujo, desde la extracciÃ³n de archivos CSV hasta la visualizaciÃ³n final de mÃ©tricas y KPIs.

---

## ðŸ§± Estructura del proyecto

El repositorio sigue una arquitectura modular de tipo **Data Warehouse Pipeline**, organizada por etapas y responsabilidades, para facilitar la trazabilidad, mantenimiento y escalabilidad del flujo de datos.

```text
EcoBottle-AR/
â”‚
â”œâ”€â”€ assets/             â†’ Recursos grÃ¡ficos (diagramas, capturas, Ã­conos)
â”‚
â”œâ”€â”€ docs/               â†’ DocumentaciÃ³n tÃ©cnica
â”‚   â””â”€â”€ data_dictionary.md
â”‚
â”œâ”€â”€ etl/                â†’ Scripts del proceso ETL (Extract, Transform, Load)
â”‚   â”œâ”€â”€ config/         â†’ ParÃ¡metros, rutas y variables globales
â”‚   â”œâ”€â”€ extract/        â†’ Lectura de fuentes RAW (.csv)
â”‚   â”œâ”€â”€ transform/      â†’ Limpieza, joins y creaciÃ³n de dimensiones/hechos
â”‚   â”œâ”€â”€ load/           â†’ Carga al Data Warehouse (SQLite)
â”‚   â”œâ”€â”€ quality/        â†’ Validaciones y controles de integridad
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ run_pipeline.py â†’ Script principal del pipeline
â”‚
â”œâ”€â”€ raw/                â†’ Archivos fuente (.csv) del ecosistema de datos
â”‚
â”œâ”€â”€ warehouse/          â†’ Resultado final del ETL
â”‚   â”œâ”€â”€ dim/            â†’ Tablas de dimensiones transformadas
â”‚   â”œâ”€â”€ fact/           â†’ Tablas de hechos transformadas
â”‚
â”œâ”€â”€ requirements.txt    â†’ Dependencias del entorno Python
â”œâ”€â”€ LICENSE             â†’ Licencia del proyecto (MIT)
â””â”€â”€ README.md           â†’ DocumentaciÃ³n principal del repositorio
```
---

## ðŸŒŸ Modelo de Datos

El proyecto se estructura en dos niveles de modelado complementarios:

### ðŸ”¹ Modelo Entidadâ€“RelaciÃ³n (DER)
Representa el **modelo lÃ³gico original** de las fuentes de datos operacionales, con todas las entidades del ecosistema comercial (clientes, pedidos, productos, pagos, envÃ­os, sesiones, etc.).  
Este modelo fue el punto de partida para comprender las relaciones entre las tablas y definir las claves de negocio.

![DER original](assets/DER.png)

> *El diagrama DER muestra las entidades base y sus relaciones antes de la transformaciÃ³n hacia el modelo dimensional.*

---

### ðŸ”¹ Modelo Estrella (Data Warehouse)
A partir del DER, se construyÃ³ el **modelo dimensional** siguiendo la metodologÃ­a de **Ralph Kimball**, diseÃ±ado para optimizar la consulta analÃ­tica y la agregaciÃ³n de mÃ©tricas.

El **esquema estrella** consolida los procesos clave del negocio en varias *fact tables*, conectadas a dimensiones comunes como tiempo, cliente, producto, tienda y canal.

**Hechos principales:**
- `FactSalesOrderItem` â€“ detalle de ventas por producto y fecha.  
- `FactPayment` â€“ pagos realizados por cliente.  
- `FactShipment` â€“ envÃ­os procesados y costos asociados.  
- `FactWebSession` â€“ sesiones web de usuarios.  
- `FactNpsResponse` â€“ respuestas de encuestas de satisfacciÃ³n (NPS).

**Dimensiones comunes:**
- `DimCalendar`, `DimCustomer`, `DimProduct`, `DimStore`, `DimChannel`, `DimProvince`, `DimAddress`, entre otras.

### ðŸ§© Diagrama del Modelo Estrella
El siguiente diagrama, generado con **Mermaid**, representa la estructura final del Data Warehouse, donde las tablas de hechos se conectan con sus respectivas dimensiones:
```mermaid
erDiagram
  %% =========================
  %% RELACIONES (segÃºn tu ETL)
  %% =========================
  DimCalendar ||--o{ FactSalesOrder        : date_sk
  DimCalendar ||--o{ FactSalesOrderItem    : date_sk
  DimCalendar ||--o{ FactPayment           : date_sk
  DimCalendar ||--o{ FactShipment          : shipped_date_sk
  DimCalendar ||--o{ FactShipment          : delivered_date_sk
  DimCalendar ||--o{ FactWebSession        : date_sk
  DimCalendar ||--o{ FactNpsResponse       : date_sk

  DimCustomer ||--o{ FactSalesOrder        : customer_sk
  DimCustomer ||--o{ FactWebSession        : customer_id
  DimCustomer ||--o{ FactNpsResponse       : customer_id

  DimChannel  ||--o{ FactSalesOrder        : channel_sk
  DimChannel  ||--o{ FactSalesOrderItem    : channel_sk
  DimChannel  ||--o{ FactWebSession        : channel_sk
  DimChannel  ||--o{ FactNpsResponse       : channel_sk

  DimProduct  ||--o{ FactSalesOrderItem    : product_sk
  DimStore    ||--o{ FactSalesOrder        : store_id_src

  DimProvince ||--o{ DimStore              : province_sk
  DimProvince ||--o{ FactSalesOrder        : province_sk
  DimProvince ||--o{ FactShipment          : province_sk

  DimAddress  ||--o{ DimCustomer           : address_id_src

  %% ================
  %% DIMENSIONES
  %% ================
  DimCalendar {
    int    date_sk PK
    date   full_date
    int    year
    int    quarter
    int    month
    string month_name
    int    week
    int    dow
    bool   is_month_end
  }

  DimProvince {
    int    province_sk PK
    string province_name
    string code
  }

  DimChannel {
    int    channel_sk PK
    string code
    string name
  }

  DimStore {
    int    store_id_src PK
    string store_name
    int    province_sk FK
  }

  DimProduct {
    int    product_sk PK
    int    product_id_src
    string sku
    string product_name
    string category
    float  list_price
    string status
  }

  DimCustomer {
    int    customer_sk PK
    int    customer_id_src
    string full_name
    string email
    int    province_sk FK
    int    address_id_src FK
    date   created_at
  }

  DimAddress {
    int    address_id_src PK
    string line1
    string city
    string postal_code
    int    province_sk FK
  }

  %% ===========
  %% HECHOS
  %% ===========
  FactSalesOrder {
    int    order_id PK
    int    date_sk FK
    int    channel_sk FK
    int    customer_sk FK
    int    store_id_src FK
    int    province_sk FK
    float  subtotal
    float  tax_amount
    float  shipping_fee
    float  total_amount
    string status
    string currency_code
  }

  FactSalesOrderItem {
    int    order_item_id PK
    int    order_id FK
    int    date_sk FK
    int    channel_sk FK
    int    product_sk FK
    int    quantity
    float  unit_price
    float  discount_amount
    float  line_total
  }

  FactPayment {
    int    payment_id PK
    int    order_id FK
    int    date_sk FK
    string method
    string status
    float  amount
  }

  FactShipment {
    int    shipment_id PK
    int    shipped_date_sk FK
    int    delivered_date_sk FK
    int    order_id FK
    int    province_sk FK
    string carrier
    string status
    float  lead_time_days
  }

  FactWebSession {
    int    session_id PK
    int    date_sk FK
    int    channel_sk FK
    int    customer_id
    string device
    string source
  }

  FactNpsResponse {
    int    nps_id PK
    int    date_sk FK
    int    channel_sk FK
    int    customer_id
    int    score
    bool   is_detractor
    bool   is_passive
    bool   is_promoter
    string comment
  }
````

## âš™ï¸ Pipeline ETL

El proceso ETL de **EcoBottle AR** automatiza la creaciÃ³n del Data Warehouse a partir de los archivos fuente en formato `.csv`.  
EstÃ¡ desarrollado en **Python** y estructurado en mÃ³dulos independientes que reflejan las tres fases clÃ¡sicas del pipeline: **Extract**, **Transform** y **Load**.

### 1ï¸âƒ£ Extract
En esta etapa se realiza la **lectura de las fuentes RAW** desde la carpeta `/raw`, aplicando validaciones bÃ¡sicas de formato y consistencia (por ejemplo: columnas esperadas, tipos de datos, duplicados y valores nulos crÃ­ticos).  

Los datos se cargan en estructuras temporales de `pandas.DataFrame` listas para ser transformadas.

```bash
python -m etl.extract
```

### 2ï¸âƒ£ Transform
AquÃ­ se lleva a cabo la limpieza, estandarizaciÃ³n e integraciÃ³n de todas las tablas.
Se generan las claves sustitutas (surrogate keys), se aplican normalizaciones de texto y se unifican los dominios de provincias, canales, productos y fechas.
TambiÃ©n se construyen las tablas de dimensiones y hechos a partir de las relaciones del modelo DER.
```bash
python -m etl.transform 
```
Principales transformaciones:
- ConversiÃ³n de tipos y manejo de nulos
- CreaciÃ³n de claves primarias y forÃ¡neas
- Join entre pedidos, clientes, productos, canales y envÃ­os
- CÃ¡lculo de mÃ©tricas derivadas (monto, cantidad, ticket promedio, etc.)
- NormalizaciÃ³n de fechas en una DimCalendar reutilizable

### 3ï¸âƒ£ Load
Los datos finales se cargan en la carpeta /warehouse, generando una base SQLite con la estructura del modelo estrella (star_schema.sql).
El proceso asegura la integridad referencial entre dimensiones y hechos, y deja el dataset listo para conectarse a Looker Studio o cualquier herramienta BI.
```bash
python -m etl.load
```

### â–¶ï¸ EjecuciÃ³n completa del pipeline
Ejecuta los tres pasos anteriores en secuencia, registrando logs del proceso:
```bash
python -m etl.run_pipeline
```

> *Este script orquesta todas las fases del ETL en orden secuencial y deja registrada la trazabilidad en consola (logging).*

## ðŸ“˜ Diccionario de Datos

El **diccionario de datos** documenta los campos, claves y dominios del modelo de datos, sirviendo como referencia para el mantenimiento y anÃ¡lisis del Data Warehouse.

Incluye:
- DescripciÃ³n funcional de cada tabla (dimensiÃ³n o hecho).  
- Significado de cada campo y su unidad de medida.  
- IdentificaciÃ³n de **claves primarias (PK)** y **forÃ¡neas (FK)**.  
- Dominios vÃ¡lidos y valores controlados (por ejemplo: provincias, canales, categorÃ­as).  
- Supuestos de negocio aplicados durante la transformaciÃ³n (por ejemplo: asignaciÃ³n de provincias faltantes, codificaciÃ³n de fechas, categorizaciÃ³n de productos).

El documento completo se encuentra en:

ðŸ“„ [`docs/data_dictionary.md`](docs/data_dictionary.md)

> *Este archivo centraliza toda la metadata del modelo estrella y sirve como insumo tÃ©cnico para futuras integraciones o validaciones de calidad.*

El **diccionario** complementa al **DER original** y al **modelo estrella**, asegurando la trazabilidad entre el origen (archivos CSV) y las estructuras finales del Data Warehouse.

## ðŸ“Š KPIs Principales

El modelo estrella permite calcular y analizar indicadores clave del negocio, combinando informaciÃ³n de ventas, pagos, envÃ­os, sesiones web y encuestas NPS.

Los siguientes **KPIs** se utilizan en el dashboard analÃ­tico para evaluar el rendimiento comercial y la satisfacciÃ³n del cliente:

| KPI | FÃ³rmula / Fuente | DescripciÃ³n |
|-----|------------------|--------------|
| **Ventas Totales** | `SUM(amount)` en `FactSalesOrderItem` | Valor total vendido en un perÃ­odo. |
| **Usuarios Activos** | `COUNT(DISTINCT customer_id)` | NÃºmero de clientes que realizaron al menos una compra. |
| **Ticket Promedio** | `SUM(amount) / COUNT(DISTINCT sales_order_id)` | Valor promedio por pedido. |
| **Pagos Aprobados** | `COUNT(payment_id WHERE status = 'Aprobado')` | Transacciones de pago exitosas. |
| **Costo de EnvÃ­os** | `SUM(cost)` en `FactShipment` | Gasto total asociado a entregas. |
| **NPS (Net Promoter Score)** | %Promotores âˆ’ %Detractores | Nivel de satisfacciÃ³n y lealtad del cliente. |
| **DuraciÃ³n Promedio de Sesiones** | `AVG(duration_seconds)` en `FactWebSession` | Tiempo promedio que un usuario permanece en la web. |

> *Estos indicadores se visualizan en el dashboard Looker Studio para monitorear tendencias de ventas, desempeÃ±o logÃ­stico y comportamiento del cliente.*

---

**Fuentes de datos:**
- `FactSalesOrderItem` â†’ Ventas y cantidades.  
- `FactPayment` â†’ MÃ©todos y estado de pagos.  
- `FactShipment` â†’ Costos y tiempos de entrega.  
- `FactWebSession` â†’ Actividad online.  
- `FactNpsResponse` â†’ SatisfacciÃ³n del cliente.  

---

## ðŸ“ˆ Dashboard AnalÃ­tico

A partir del modelo estrella almacenado en `warehouse/`, se desarrollÃ³ un **dashboard interactivo** en **Looker Studio**, que consolida los KPIs del negocio y permite explorar los datos desde distintas perspectivas: comercial, logÃ­stica, digital y de experiencia del cliente.

El tablero integra informaciÃ³n de todas las *fact tables* (ventas, pagos, envÃ­os, sesiones y NPS) y ofrece una visiÃ³n unificada del desempeÃ±o general de **EcoBottle AR**.

### ðŸŽ¯ Objetivo
Brindar una herramienta de anÃ¡lisis que permita:
- Monitorear el cumplimiento de objetivos comerciales.
- Analizar el comportamiento de clientes y productos.
- Identificar patrones de compra y eficiencia logÃ­stica.
- Evaluar la satisfacciÃ³n del cliente mediante NPS.

### ðŸ§­ Principales vistas
- **Ventas por provincia y canal**  
- **Ranking mensual de productos**  
- **EvoluciÃ³n del ticket promedio y volumen de ventas**  
- **Tiempos y costos de envÃ­o por zona**  
- **Actividad de usuarios en canales online**  
- **DistribuciÃ³n de NPS (Promotores / Neutros / Detractores)**  

---

ðŸ“Š **Herramienta:** [Google Looker Studio](https://lookerstudio.google.com/)  
ðŸ“· *Captura del dashboard final:*  
![Dashboard Looker Studio](assets/dashboard_preview.png)

---

## ðŸ§¾ Licencia y reconocimiento

Este proyecto fue desarrollado por **Bernardo Di Rienzo** en el marco de la Licenciatura en Ciencia de Datos de la Universidad Austral (2025).

Los archivos originales de datos (`/raw`) y el diagrama entidadâ€“relaciÃ³n (`DER`) provienen del repositorio acadÃ©mico base del profesor **Augusto Carmona**, utilizados Ãºnicamente como punto de partida para la prÃ¡ctica de modelado y anÃ¡lisis.

> Â© 2025 Bernardo Di Rienzo â€“ MIT License. Todos los derechos reservados sobre los desarrollos y documentaciÃ³n propios.

